{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "808ef8a7",
   "metadata": {},
   "source": [
    "# Differentiable Programming \n",
    "\n",
    "This tutorial aims to take you through the basics of writing a differentiable program using PyTorch and to highlight some of the ways to use this type of tool. If you've trained a neural network, you've already written a differentiable program! But we will try to show the power of generalizing beyond neural networks, and to bring out some subtleties that you don't usually have to worry about. The tutorial is structured as follows:\n",
    "\n",
    "1. **Introduction: Parameter Fitting** We begin with a simple example of fitting parameters of a differentiable physics simulator using gradient-based optimization. The ingredients here should look familiar, just composed in a different way.\n",
    "\n",
    "\n",
    "2. **Tips and Tricks: Hard Edges and You** Not everything is (nicely) differentiable! What are some ways to deal with that?\n",
    "\n",
    "\n",
    "3. **Extensions: Differentiable Pipelines** A powerful use of differentiable programming is to use physics code in conjunction with neural networks. We present a simple example how how these elements might be composed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4304c0dc",
   "metadata": {},
   "source": [
    "## Introduction: Parameter Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fa5d0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da30922",
   "metadata": {},
   "source": [
    "### A simple physics model\n",
    "\n",
    "Suppose we want to simulate the motion of a ball. More specifically, for each time $t$, we would like to know the position, $\\vec{x}$ of an object, perhaps under some acceleration $\\vec{a}$.\n",
    "\n",
    "Kinematic equations tell us:\n",
    "\\begin{equation}\n",
    "\\vec{x}(t) = \\vec{x}_{0} + \\vec{v}_{0}t + \\frac{1}{2}\\vec{a}t^2\n",
    "\\end{equation}\n",
    "\n",
    "where $\\vec{x}_{0}$ and $\\vec{v}_{0}$ are the initial position/velocity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9d787e",
   "metadata": {},
   "source": [
    "In code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f059f57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def position(t, a, x0, v0):\n",
    "    return x0 + v0*t + 0.5*a*t**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec31078",
   "metadata": {},
   "source": [
    "Sticking to 2d for visualization purposes, we can run a simple simulation to get the trajectory of an object under the influence of gravity, namely:\n",
    "\n",
    "$\\vec{a}$ = (0, -9.8), in units of $m / s^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d86a83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Acceleration due to gravity (-9.8 m/s^2 in negative y direction)\n",
    "a = np.asarray([0, -9.8])\n",
    "\n",
    "#Initial velocity for fun (m / s)\n",
    "v0 = np.asarray([5, 10])\n",
    "\n",
    "#Start at (0,10) [m]\n",
    "x0 = np.asarray([0,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82de878",
   "metadata": {},
   "source": [
    "We then can create a trajectory for the object by \"measuring\" position at a set of time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab98e64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run the simulation: 100 timesteps\n",
    "time_steps = np.linspace(0, 5, 10)\n",
    "\n",
    "trajectory = []\n",
    "for t_step in time_steps:\n",
    "    xt = position(t_step, a, x0, v0)\n",
    "    trajectory.append(xt)\n",
    "    \n",
    "trajectory = np.asarray(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af20b42",
   "metadata": {},
   "source": [
    "For fun, let's animate how this might look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12068ea4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xlim = [trajectory[:, 0].min(), trajectory[:, 0].max()]\n",
    "ylim = [trajectory[:, 1].min(), trajectory[:, 1].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f664f949",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "dot, = ax.plot([x0[0]], [x0[1]], 'ro')\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_xlabel('x [m]')\n",
    "ax.set_ylabel('y [m]')\n",
    "\n",
    "def animate(time_step):\n",
    "    dot.set_data([trajectory[time_step, 0]], [trajectory[time_step, 1]])\n",
    "    return dot,\n",
    "\n",
    "anim = FuncAnimation(fig, animate, frames=len(time_steps), repeat=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747b45be",
   "metadata": {},
   "source": [
    "Or, just plotting all time steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9ce436-01f2-4ecc-942e-83dffc3ec1bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e952a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(trajectory[:, 0], trajectory[:, 1], '-o')\n",
    "plt.xlabel('x [m]')\n",
    "plt.ylabel('y [m]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c55387",
   "metadata": {},
   "source": [
    "We now have a physics simulator (albeit a simple one)! \n",
    "\n",
    "### Terminology: \n",
    "In the ML context, we call the above simulator our **forward model**, namely: we start from some set of conditions, and we use the forward model to get some result.\n",
    "\n",
    "### Today: \n",
    "Imagine we know the forward model, and we have some dataset. In a real context, this could correspond to experimental data, where the forward model is the physics simulation. How can we use the forward model to gain insight into the experimental data? More particularly, can we use the forward model to extract physical parameters from the real dataset? This is where differentiable simulation can be useful! Notable examples: detector calibration, physics reconstruction.\n",
    "\n",
    "### In our case:\n",
    "Suppose we're given some kinematic trajectory, but we don't know the initial velocity? Or the acceleration? Or the initial position? A differentiable kinematics simulator provides an easy tool to extract these parameters. Let's see how this works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac98b8f",
   "metadata": {},
   "source": [
    "### Step 1: Differentiable forward model\n",
    "\n",
    "We'll use PyTorch here, but the same concepts apply to any framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e218f75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed9ef90",
   "metadata": {},
   "source": [
    "### Convenience step: Vectorization\n",
    "PyTorch is a tensor library! In principle, for loops are ok, but often slower/less nice for GPU acceleration.\n",
    "\n",
    "Our data: single 2D kinematic trajectory with measurements at `n_steps` time steps $\\implies$ the output of the simulation should have shape `(n_steps, 2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ee3c0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trajectory.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918ad9f6",
   "metadata": {},
   "source": [
    "Repeating here for clarity, but: position function can stay the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27b610e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def position(t, a, x0, v0):\n",
    "    return x0 + v0*t + 0.5*a*t**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61289e41",
   "metadata": {},
   "source": [
    "However, we need to adjust the shapes to work.\n",
    "\n",
    "Think of time as the zeroth dimension, spatial coordinates as the first dimension. Then, if we define our parameters as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090e9a12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Acceleration due to gravity (-9.8 m/s^2 in negative y direction)\n",
    "a = torch.tensor([0, -9.8])[None, :]\n",
    "\n",
    "#Initial velocity for fun (m / s)\n",
    "v0 = torch.tensor([5, 10])[None, :]\n",
    "\n",
    "#Start at (0,10) [m]\n",
    "x0 = torch.tensor([0,10])[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1006db38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e86ec0",
   "metadata": {},
   "source": [
    "and our timesteps as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93823e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run the simulation: 10 timesteps\n",
    "time_steps = torch.linspace(0, 5, 10)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79988906",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_steps.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695d9921",
   "metadata": {},
   "source": [
    "Broadcasting will then give us the desired shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b3d155",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trajectory = position(time_steps, a, x0, v0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d623e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(trajectory.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efea361",
   "metadata": {},
   "source": [
    "and we haven't changed any values, so the output should be identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f33ad6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(trajectory[:, 0], trajectory[:, 1], '-o')\n",
    "plt.xlabel('x [m]')\n",
    "plt.ylabel('y [m]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b100cb20",
   "metadata": {},
   "source": [
    "### Summary: \n",
    "\n",
    "Things are easy for us today! Our forward model is a single function, `position`, which takes in measurement times, acceleration, initial position, and initial velocity, and returns a trajectory ($x$ and $y$ positions at each time step).\n",
    "\n",
    "Note that this is trivially differentiable -- we only use addition and multiplication! A more complicated simulation might require more tricks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a28f01e",
   "metadata": {},
   "source": [
    "### Step 2: \"Take\" some data\n",
    "\n",
    "For today, we'll be using a single trajectory with some given parameter values as our target. This is the most perfect case -- our forward model is exact and we know the true parameter values. Real life is more complicated, but everybody starts somewhere!\n",
    "\n",
    "Let's use the same trajectory as above, but recording for posterity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1300c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Acceleration due to gravity (-9.8 m/s^2 in negative y direction)\n",
    "a_target = torch.tensor([0, -9.8])[None, :]\n",
    "\n",
    "#Initial velocity for fun (m / s)\n",
    "v0_target = torch.tensor([5, 10])[None, :]\n",
    "\n",
    "#Start at (0,10) [m]\n",
    "x0_target = torch.tensor([0,10])[None, :]\n",
    "\n",
    "#Data taken at 10 known timesteps from 0 to 5 s\n",
    "time_steps = torch.linspace(0, 5, 10)[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196763fc",
   "metadata": {},
   "source": [
    "Target is the corresponding trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f464da7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trajectory_target = position(time_steps, a_target, x0_target, v0_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d456cff",
   "metadata": {},
   "source": [
    "### Step 3: Set up parameter gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20d705f",
   "metadata": {},
   "source": [
    "To start, let's try to solve for the **initial velocity**. We need to set up a new forward model with an initial guess. We can then iterate over the new model parameters.\n",
    "\n",
    "\n",
    "We assume only v0 changes for now, so everything else is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d052ad3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Acceleration due to gravity (-9.8 m/s^2 in negative y direction)\n",
    "a_guess = torch.tensor([0, -9.8])[None, :]\n",
    "\n",
    "#Start at (0,10) [m]\n",
    "x0_guess = torch.tensor([0.,10.])[None, :]\n",
    "\n",
    "#Run the simulation: 10 timesteps\n",
    "time_steps = torch.linspace(0, 5, 10)[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c46b41f",
   "metadata": {},
   "source": [
    "And we don't know v0, so let's initialize to (0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d48911",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Initial velocity for fun (m / s)\n",
    "v0_guess = torch.tensor([0., 0.])[None, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b622a2",
   "metadata": {},
   "source": [
    "Now we need a training loop. There are two important pieces:\n",
    "\n",
    "1. A **loss function**: in this case the easiest is just the Mean Squared Error between the output of the \"guess\" forward model and the target trajectory.\n",
    "\n",
    "2. An **optimizer**: how do we change parameters to try to fit the trajectory? Let's start with a simple gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818b562c",
   "metadata": {},
   "source": [
    "Recall that gradient descent for some parameter (or vector of parameters) $\\theta$ at training step $i$ has update rule:\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_{i+1} = \\theta_{i} - \\eta\\cdot\\nabla_{\\theta}f(\\theta_i)\n",
    "\\end{equation}\n",
    "\n",
    "where $f$ is our objective function, and $\\eta$ is a _learning rate_ which controls the size of the gradient step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8032775e",
   "metadata": {},
   "source": [
    "To do gradient descent, we need to keep track of gradients. In PyTorch this is done with setting `requires_grad = True` for the relevant parameters, either as an argument to `torch.tensor` in the initialization, or after the fact, like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57c515c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "v0_guess.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ae9243",
   "metadata": {},
   "source": [
    "Gradients of `v0_guess` are accessible via the attribute `v0_guess.grad`. Right now, this isn't populated (`v0_guess` hasn't interacted with the physics model/we haven't done the backward autodiff pass). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcde235e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(v0_guess.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dd9fb2",
   "metadata": {},
   "source": [
    "This may all be somewhat familiar from training neural networks, and it should be -- it's doing the same thing! However many of these details are abstracted away. We'll set up an explicit example below (before we do some abstraction ourselves)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97019b2",
   "metadata": {},
   "source": [
    "### The training loop\n",
    "\n",
    "There are a few steps in the typical autodiff training loop. First, we need to initialize the parameters we want to fit -- we already did this above, but just to keep everything together, let's repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a244d02",
   "metadata": {},
   "source": [
    "### Fixed parameters:\n",
    "For simplicity, we're only fitting v0 for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515393a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Acceleration due to gravity (-9.8 m/s^2 in negative y direction)\n",
    "a_guess = torch.tensor([0, -9.8])[None, :]\n",
    "\n",
    "#Start at (0,10) [m]\n",
    "x0_guess = torch.tensor([0.,10.])[None, :]\n",
    "\n",
    "#Run the simulation: 10 timesteps\n",
    "time_steps = torch.linspace(0, 5, 10)[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef46abc",
   "metadata": {},
   "source": [
    "### Not fixed parameters:\n",
    "We want to solve for `v0_guess`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d3d2e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Initial velocity for fun (m / s)\n",
    "v0_guess = torch.tensor([0., 0.])[None, :]\n",
    "v0_guess.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5327b96f",
   "metadata": {},
   "source": [
    "Then we make a guess with the initial parameters. This is done with a simple pass through the forward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2054c4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trajectory_guess = position(time_steps, a_guess, x0_guess, v0_guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f75535",
   "metadata": {},
   "source": [
    "Then compute the loss between the guessed trajectory and the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9f09bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = ((trajectory_target - trajectory_guess)**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adbe3dd",
   "metadata": {},
   "source": [
    "and let PyTorch accumulate gradients with a backward pass. Note that the backward pass starts at the loss calculation -- this is the thing we want to minimize with gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0c049f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3dbb28",
   "metadata": {},
   "source": [
    "`v0_guess` now has an associated gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebd0546",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(v0_guess.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a399db",
   "metadata": {},
   "source": [
    "and we expect a velocity of order 10, so let's define a learning rate so that our updates are of appropriate size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d65bcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40d6997",
   "metadata": {},
   "source": [
    "This allows us to actually do the update of `v0_guess`. We don't want to keep track of gradients through this update, so we wrap this in a `with torch.no_grad()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d6b946",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    v0_guess -= lr*v0_guess.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7e1851",
   "metadata": {},
   "source": [
    "`v0_guess` now has an updated value, so we can repeat. Note that, similar to training a neural network, we need to zero out the parameter gradients before the next iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de7b7f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(v0_guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0aa036",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise 1: Combine all of that into a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6354a898",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize training parameters\n",
    "\n",
    "# Learning rate -- controls the size of the optimization step\n",
    "\n",
    "\n",
    "# Keep track of training and losses \n",
    "training_path = []\n",
    "losses = []\n",
    "v0_steps = []\n",
    "\n",
    "# Do 10 parameter updates\n",
    "for epoch in range(10):  \n",
    "    \n",
    "    # Zero out the gradients\n",
    "    if v0_guess.grad is not None:\n",
    "        v0_guess.grad.zero_()\n",
    "        \n",
    "    # Guess with current parameters\n",
    "    \n",
    "    \n",
    "    # Calculate loss\n",
    "\n",
    "    \n",
    "    # Compute gradients\n",
    "\n",
    "\n",
    "    # Update for next loop\n",
    "\n",
    "        \n",
    "    # Monitoring and history tracking\n",
    "    print(f\"Iter {epoch}, Loss: {loss.detach().item()}\")\n",
    "    training_path.append(trajectory_guess.detach())\n",
    "    losses.append(loss.item())\n",
    "    v0_steps.append(v0_guess.detach().clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c6a837",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel('Training Iteration')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9aad59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for step, traj in enumerate(training_path):\n",
    "    plt.plot(traj[:, 0], traj[:, 1], label=f'Guess {step}')\n",
    "plt.plot(trajectory_target[:, 0], trajectory_target[:, 1], c='k', label='Target')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('x [m]')\n",
    "plt.ylabel('y [m]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7a4de5",
   "metadata": {},
   "source": [
    "As expected from the loss, we get closer to the target trajectory with each iteration. Looking at the values of v0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2330a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i_step, step in enumerate(v0_steps):\n",
    "    plt.scatter(step[:, 0], step[:, 1], label=f'Step {i_step}')\n",
    "plt.scatter(5, 10, c='k', label='Target')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlabel('$v_{0}^{x}$ [m / s]')\n",
    "plt.ylabel('$v_{0}^{y}$ [m / s]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7277ee4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(torch.cat(v0_steps)[:, 0], c='r', label='Fit')\n",
    "plt.plot([0, len(v0_steps)], [v0_target[0][0]]*2, c='k', ls='dashed', label='True')\n",
    "plt.xlabel('Training Iterations')\n",
    "plt.ylabel('$v_{0}^x$')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(torch.cat(v0_steps)[:, 1], c='r', label='Fit')\n",
    "plt.plot([0, len(v0_steps)], [v0_target[0][1]]*2, c='k', ls='dashed', label='True')\n",
    "plt.xlabel('Training Iterations')\n",
    "plt.ylabel('$v_{0}^y$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5b2fe4",
   "metadata": {},
   "source": [
    "we recover the target parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad440929",
   "metadata": {},
   "source": [
    "## Slightly more complicated -- more parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0748bb8",
   "metadata": {},
   "source": [
    "Let's try to fit all parameters! The training loop is basically the same, just with more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ce6d50-b7ad-4f89-85f1-bef83a2ad1af",
   "metadata": {},
   "source": [
    "### Exercise 2: Fill in the gaps for the other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc4aed8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize training parameters. This time we want gradients for a, x0, and v0\n",
    "v0_guess = torch.tensor([0., 0.])[None, :]\n",
    "# Add in a_guess, x0_guess\n",
    "\n",
    "a_guess.requires_grad = True\n",
    "# Add in a_guess, x0_guess\n",
    "\n",
    "# Learning rate -- controls the size of the optimization step\n",
    "lr = 0.1\n",
    "\n",
    "# Keep track of training and losses \n",
    "training_path = []\n",
    "losses = []\n",
    "\n",
    "# Do 10 parameter updates\n",
    "for epoch in range(10):   \n",
    "    \n",
    "    for param in [v0_guess, x0_guess, a_guess]:\n",
    "        if param.grad is not None:\n",
    "            param.grad.zero_()\n",
    "\n",
    "    # Guess with current parameters\n",
    "    trajectory_guess = position(time_steps, a_guess, x0_guess, v0_guess)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = ((trajectory_target - trajectory_guess)**2).mean()\n",
    "    \n",
    "    # Compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    print(f\"Iter {epoch}, Loss: {loss.detach().item()}\")\n",
    "    \n",
    "    # Update for next loop -- simultaneous update of all three\n",
    "    with torch.no_grad():\n",
    "        v0_guess -= lr*v0_guess.grad\n",
    "        # Add in a_guess, x0_guess\n",
    "\n",
    "    # Store training history.\n",
    "    training_path.append(trajectory_guess.detach())\n",
    "    losses.append(loss.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9708a5ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel('Training Iteration')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f8e3c3",
   "metadata": {},
   "source": [
    "### Uh oh! Something went wrong! The loss should be going down!\n",
    "\n",
    "Usually when this happens, the learning rate is too big -- the optimization procedure doesn't converge to a minimum because we're \"bouncing around\" the parameter space.\n",
    "\n",
    "#### Solutions:\n",
    "1. Guess and check: try decreasing learning rate. Often helpful to look at the magnitude of the corresponding gradients to get an idea of appropriate magnitudes\n",
    "2. Use a better optimizer (e.g. something adaptive like Adam)\n",
    "\n",
    "In our case, guessing should be sufficient. Note that _decreasing_ the size of our update steps means that we should _increase_ the number of training iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3bae13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize training parameters. This time we want gradients for a, x0, and v0\n",
    "a_guess = torch.tensor([0., 0.])[None, :]\n",
    "x0_guess = torch.tensor([0.,0.])[None, :]\n",
    "v0_guess = torch.tensor([0., 0.])[None, :]\n",
    "\n",
    "a_guess.requires_grad = True\n",
    "x0_guess.requires_grad = True\n",
    "v0_guess.requires_grad = True\n",
    "\n",
    "# Learning rate -- controls the size of the optimization step: decrease from 0.1 to 0.01\n",
    "lr = 0.01\n",
    "\n",
    "# Keep track of training and losses \n",
    "training_path = []\n",
    "losses = []\n",
    "\n",
    "# Do 10 parameter updates\n",
    "for epoch in tqdm(range(10000)):   \n",
    "    \n",
    "    for param in [v0_guess, x0_guess, a_guess]:\n",
    "        if param.grad is not None:\n",
    "            param.grad.zero_()\n",
    "\n",
    "    # Guess with current parameters\n",
    "    trajectory_guess = position(time_steps, a_guess, x0_guess, v0_guess)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = ((trajectory_target - trajectory_guess)**2).mean()\n",
    "    \n",
    "    # Compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update for next loop -- simultaneous update of all three\n",
    "    with torch.no_grad():\n",
    "        v0_guess -= lr*v0_guess.grad\n",
    "        x0_guess -= lr*x0_guess.grad\n",
    "        a_guess -= lr*a_guess.grad\n",
    "\n",
    "    # Store training history.\n",
    "    training_path.append(trajectory_guess.detach())\n",
    "    losses.append(loss.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97685b62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel('Training Iteration')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10477606",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(losses[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974be200",
   "metadata": {},
   "source": [
    "Much healthier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78b80cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for step, traj in enumerate(training_path):\n",
    "    if step % 10 == 0:\n",
    "        plt.plot(traj[:, 0], traj[:, 1], label=f'Guess {step}')\n",
    "plt.plot(trajectory_target[:, 0], trajectory_target[:, 1], c='k', label='Target')\n",
    "#plt.legend()\n",
    "plt.xlabel('x [m]')\n",
    "plt.ylabel('y [m]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d99acdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(training_path[-1][:, 0], training_path[-1][:, 1], label='Final Iter')\n",
    "plt.plot(trajectory_target[:, 0], trajectory_target[:, 1], c='k', label='Target')\n",
    "plt.legend()\n",
    "plt.xlabel('x [m]')\n",
    "plt.ylabel('y [m]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88afc44f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Final v0: {v0_guess.detach()[0]}, Target: {v0_target[0]}')\n",
    "print(f'Final x0: {x0_guess.detach()[0]}, Target: {x0_target[0]}')\n",
    "print(f'Final a: {a_guess.detach()[0]}, Target: {a_target[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2fe9b1",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "We've manually defined our loss function and optimizer, but using built in PyTorch stuff generalizes much better! Rewriting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d8f57c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Same MSE loss from PyTorch\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# If we want to initialize at 0's we should use torch.zeros\n",
    "a_guess = torch.zeros((1, 2), requires_grad=True)\n",
    "x0_guess = torch.zeros((1, 2), requires_grad=True)\n",
    "v0_guess = torch.zeros((1, 2), requires_grad=True)\n",
    "\n",
    "# Torch has a bunch of built in optimizers. Stick with gradient descent.\n",
    "optimizer = torch.optim.SGD([a_guess, x0_guess, v0_guess], lr=0.01)\n",
    "\n",
    "# Same simple history tracking\n",
    "training_path = []\n",
    "losses = []\n",
    "for epoch in tqdm(range(10000)):\n",
    "    # This line replaces the per-parameter gradient zeroing\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward model done the same way\n",
    "    trajectory_guess = position(time_steps, a_guess, x0_guess, v0_guess)\n",
    "    \n",
    "    # Pass to loss function\n",
    "    loss = loss_fn(trajectory_target, trajectory_guess)\n",
    "    \n",
    "    # Backward pass done the same way\n",
    "    loss.backward()\n",
    "\n",
    "    # The optimizer takes care of the gradient steps\n",
    "    optimizer.step()\n",
    "        \n",
    "    losses.append(loss.detach())\n",
    "    training_path.append(trajectory_guess.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba4324b",
   "metadata": {},
   "source": [
    "Results are completely identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0c8231",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel('Training Iteration')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10d92d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e932aa3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for step, traj in enumerate(training_path):\n",
    "    if step % 10 == 0:\n",
    "        plt.plot(traj[:, 0], traj[:, 1], label=f'Guess {step}')\n",
    "plt.plot(trajectory_target[:, 0], trajectory_target[:, 1], c='k', label='Target')\n",
    "#plt.legend()\n",
    "plt.xlabel('x [m]')\n",
    "plt.ylabel('y [m]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86db36e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(training_path[-1][:, 0], training_path[-1][:, 1], label='Final Iter')\n",
    "plt.plot(trajectory_target[:, 0], trajectory_target[:, 1], c='k', label='Target')\n",
    "plt.legend()\n",
    "plt.xlabel('x [m]')\n",
    "plt.ylabel('y [m]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23108136",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Final v0: {v0_guess.detach()[0]}, Target: {v0_target[0]}')\n",
    "print(f'Final x0: {x0_guess.detach()[0]}, Target: {x0_target[0]}')\n",
    "print(f'Final a: {a_guess.detach()[0]}, Target: {a_target[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3171f3c0",
   "metadata": {},
   "source": [
    "Using PyTorch built-ins let's us easily explore other optimizers (e.g. Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c17f86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Same MSE loss from PyTorch\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# If we want to initialize at 0's we should use torch.zeros\n",
    "a_guess = torch.zeros((1, 2), requires_grad=True)\n",
    "x0_guess = torch.zeros((1, 2), requires_grad=True)\n",
    "v0_guess = torch.zeros((1, 2), requires_grad=True)\n",
    "\n",
    "# Torch has a bunch of built in optimizers. Stick with gradient descent.\n",
    "optimizer = torch.optim.Adam([a_guess, x0_guess, v0_guess], lr=1)\n",
    "\n",
    "# Same simple history tracking\n",
    "training_path = []\n",
    "losses = []\n",
    "for epoch in tqdm(range(1000)):\n",
    "    # This line replaces the per-parameter gradient zeroing\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward model done the same way\n",
    "    trajectory_guess = position(time_steps, a_guess, x0_guess, v0_guess)\n",
    "    \n",
    "    # Pass to loss function\n",
    "    loss = loss_fn(trajectory_target, trajectory_guess)\n",
    "    \n",
    "    # Backward pass done the same way\n",
    "    loss.backward()\n",
    "\n",
    "    # The optimizer takes care of the gradient steps\n",
    "    optimizer.step()\n",
    "        \n",
    "    losses.append(loss.detach())\n",
    "    training_path.append(trajectory_guess.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc32c88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f48fdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Final v0: {v0_guess.detach()[0]}, Target: {v0_target[0]}')\n",
    "print(f'Final x0: {x0_guess.detach()[0]}, Target: {x0_target[0]}')\n",
    "print(f'Final a: {a_guess.detach()[0]}, Target: {a_target[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d9b4f2",
   "metadata": {},
   "source": [
    "## Tips and Tricks: Hard Edges and You\n",
    "\n",
    "We now know how to fit parameters of a physics simulation using differentiable programming. But...our simulation is quite simple. What happens if it is more complex? What about if it's not trivially differentiable? \n",
    "\n",
    "\n",
    "Let's walk through an example where we want to make a cut on an observable to maximize a single binned significance (\"cut and count\"). For simplicity, we set this up as follows:\n",
    "\n",
    "\n",
    "Suppose we have 500 total signal events ($s$) and 10000 total background events ($b$) with mass distributions peaked in distinct spots ($\\mathbb{E}[m_{s}] = 1100$ GeV and $\\mathbb{E}[m_{b}] = 700$ GeV). We want to find a cut value on $m$ that maximizes $s/\\sqrt{b}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e29ad9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the toy data\n",
    "n_s = 500\n",
    "n_b = 10000\n",
    "\n",
    "torch.random.manual_seed(3)\n",
    "m_s = 1000*torch.normal(torch.ones(n_s)*0.6, torch.ones(n_s)*0.05)+500\n",
    "m_b = 1000*torch.normal(torch.ones(n_b)*0.2, torch.ones(n_b)*0.3)+500\n",
    "\n",
    "# Concatenate to one vector of observable values\n",
    "all_ms = torch.cat([m_s, m_b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810eefce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the distributions\n",
    "plt.hist(m_s.numpy(), bins=20, range=[0, 2000], histtype='step', ec='r', label='Signal')\n",
    "plt.hist(m_b.numpy(), bins=20, range=[0, 2000], histtype='step', ec='b', label='Background')\n",
    "\n",
    "plt.hist(all_ms.numpy(),  bins=20, range=[0, 2000], histtype='step', ec='k', label='\"Data\"')\n",
    "plt.xlabel('$m$ [GeV]')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad20c7e9",
   "metadata": {},
   "source": [
    "We want to make a cut and maximize $s/\\sqrt{b}$ for the remaining events. As an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a83e04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pick a cut value\n",
    "a = torch.tensor(500.)\n",
    "\n",
    "# Number of signal events passing cut\n",
    "n_s_pass = (m_s >= a).sum()\n",
    "\n",
    "# Number of background events passing cut\n",
    "n_b_pass = (m_b >= a).sum()\n",
    "\n",
    "# s/sqrt(b)\n",
    "print(n_s_pass/np.sqrt(n_b_pass))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94c3bb2",
   "metadata": {},
   "source": [
    "This is a 1D example, so we can construct the ground truth by just sweeping over values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defc0e6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_as = torch.linspace(500, 1500, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca60973-2841-48b2-aa77-1a9e988c492d",
   "metadata": {},
   "source": [
    "### Exercise 3: vectorization\n",
    "\n",
    "Remember our trick from before of adding singular `x[None, :]` type dimensions to make shapes work out. \n",
    "\n",
    "Specifically, we wanted to calculate over all time steps and each spatial dimension at the same time, so we had shapes like:\n",
    "```\n",
    "#Start at (0,10) [m]\n",
    "x0_target = torch.tensor([0,10])[None, :]\n",
    "\n",
    "#Data taken at 10 known timesteps from 0 to 5 s\n",
    "time_steps = torch.linspace(0, 5, 10)[:, None]\n",
    "```\n",
    "\n",
    "so that we ended up with a tensor of shape (n_time_steps, n_space) = (10, 2)\n",
    "\n",
    "Can we calclulate `n_s` in a similar way to compare all values of `m_s` with all cuts (values of `test_as`)? \n",
    "\n",
    "Specifically, we want to end up with a tensor of shape `(len(m_s), len(test_as))` which we can then sum over `dim=0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9914f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fill in lines for all_n_s and all_n_b\n",
    "\n",
    "all_sig = all_n_s/torch.sqrt(all_n_b)\n",
    "\n",
    "plt.plot(test_as, all_sig)\n",
    "plt.xlabel('Cut Values')\n",
    "plt.ylabel('$s/\\sqrt{b}$')\n",
    "plt.show()\n",
    "\n",
    "best_idx = torch.argmax(all_sig)\n",
    "print(f'Best cut: {test_as[best_idx]}, s/sqrt(b): {all_sig[best_idx]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14b1241",
   "metadata": {},
   "source": [
    "### Problem statement:\n",
    "Can we use differentiable programming + gradient-based optimization to find the cut value that maximizes $s/\\sqrt{b}$?\n",
    "\n",
    "\n",
    "Let's break down the function that we're trying to optimize (remember, autodiff is the chain rule).\n",
    "\n",
    "\n",
    "For a given value of cut $a$, event with mass $m$, we first need to check if $m$ > $a$. If yes, it contributes to the event count, if no, it doesn't.\n",
    "\n",
    "One way of representing this is via a piecewise function\n",
    "\\begin{equation}\n",
    "    p(m) = \n",
    "    \\begin{cases} \n",
    "      1 & m \\geq a \\\\\n",
    "      0 & m < a\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c8e6f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = 500.\n",
    "m_vals = torch.linspace(0, 2000, 1000)\n",
    "\n",
    "plt.plot(m_vals, (m_vals>=a).int())\n",
    "plt.xlabel('$m$ [GeV]')\n",
    "plt.ylabel('$p(m)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5285176",
   "metadata": {},
   "source": [
    "This is the form of the [Heaviside step function](https://en.wikipedia.org/wiki/Heaviside_step_function).\n",
    "\n",
    "One problem: what is the derivative, $\\frac{dp(m)}{dm}$? If we look at the form of the function, we see it's 0 everywhere (and infinite at one point) -- not very useful for optimization!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac8b6d7",
   "metadata": {},
   "source": [
    "On the code side, there's also a bit of an issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4f780e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pick a cut value\n",
    "a = torch.tensor(500., requires_grad=True)\n",
    "\n",
    "# Number of signal events passing cut\n",
    "n_s_pass = (m_s >= a).sum()\n",
    "\n",
    "# Number of background events passing cut\n",
    "n_b_pass = (m_b >= a).sum()\n",
    "\n",
    "# s/sqrt(b)\n",
    "print(n_s_pass/torch.sqrt(n_b_pass))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a36e2c1",
   "metadata": {},
   "source": [
    "Usually we have an associated grad_fn, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ca94d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a*2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f57f44d",
   "metadata": {},
   "source": [
    "But that's not the case here (and for good reason). One common approach to this type of issue is to use some _relaxation_ of the hard edge/discrete cut -- some function with nicer derivatives that is close to (but not exactly) the function of interest.\n",
    "\n",
    "For step-function like functions, a good choice is the sigmoid:\n",
    "\n",
    "\\begin{equation}\n",
    "S_{k}(x) = \\frac{1}{1+e^{-k\\cdot x}}\n",
    "\\end{equation}\n",
    "\n",
    "where $k$ is a rate parameter that controls the steepness of the transition from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb48632",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def soft_gt(x, a, rate=1.):\n",
    "    return torch.sigmoid(rate*(x-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa0d89f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = 500.\n",
    "m_vals = torch.linspace(0, 2000, 1000)\n",
    "\n",
    "plt.plot(m_vals, (m_vals>=a))\n",
    "for rate in [1e-3, 1e-2, 0.1, 1, 10]:\n",
    "    plt.plot(m_vals, soft_gt(m_vals, a, rate=rate), label=f'k={rate}')\n",
    "plt.xlabel('$m$ [GeV]')\n",
    "plt.ylabel('$p(m)$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e88cd86",
   "metadata": {},
   "source": [
    "We can tune the rate parameter to trade off between accuracy (how close we are to the hard cut) and smoothness (how good the gradients are).\n",
    "\n",
    "\n",
    "\n",
    "In context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6638e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose some rate (may need to adjust)\n",
    "rate=10.\n",
    "\n",
    "# Calculate s/sqrt(b) with the soft greater than\n",
    "all_n_s_soft = soft_gt(m_s[:, None], test_as[None, :], rate=rate).sum(dim=0)\n",
    "all_n_b_soft = soft_gt(m_b[:, None], test_as[None, :], rate=rate).sum(dim=0)\n",
    "\n",
    "all_sig_soft = all_n_s_soft/torch.sqrt(all_n_b_soft)\n",
    "\n",
    "# Compare to the exact version\n",
    "plt.plot(test_as, all_sig, label='Exact')\n",
    "plt.plot(test_as, all_sig_soft, label='Softened')\n",
    "plt.ylabel(\"$s/\\sqrt{b}$\")\n",
    "plt.xlabel(\"Cut Value [GeV]\")\n",
    "plt.show()\n",
    "print(f'Best cut, soft: {test_as[np.argmax(all_sig_soft)]}')\n",
    "print(f'Best cut, exact: {test_as[np.argmax(all_sig)]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c236429",
   "metadata": {},
   "source": [
    "Let's check that gradients flow now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f51a2c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rate=10.\n",
    "# Pick a cut value\n",
    "a = torch.tensor(500., requires_grad=True)\n",
    "\n",
    "# Number of signal events passing cut\n",
    "n_s_pass_soft = soft_gt(m_s, a, rate=rate).sum()\n",
    "\n",
    "# Number of background events passing cut\n",
    "n_b_pass_soft = soft_gt(m_b, a, rate=rate).sum()\n",
    "\n",
    "# s/sqrt(b)\n",
    "print(n_s_pass_soft/torch.sqrt(n_b_pass_soft))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171a4c84",
   "metadata": {},
   "source": [
    "Success! Let's try running an optimization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc42577-ac67-4ae2-abe2-d9c90a42523b",
   "metadata": {},
   "source": [
    "### Exercise 4: Fill in the loop\n",
    "\n",
    "Translate the previous cell into an optimization loop, similar to above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a60602d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initial guess\n",
    "\n",
    "# Optimizer: suggest using Adam with lr=1, parameter to optimize should be passed as list [a]\n",
    "\n",
    "# History tracking\n",
    "a_history = []\n",
    "losses = []\n",
    "for epoch in tqdm(range(2000)):\n",
    "    # Zero out the gradients\n",
    "    \n",
    "    \n",
    "    # Calculate s/sqrt(b)\n",
    "    # First do n_s using soft_gt(m_s, a, rate=rate).sum()\n",
    "\n",
    "    # Then calculate n_b similarly with m_b\n",
    "\n",
    "    # Then s/torch.sqrt(b)\n",
    "    \n",
    "    # Finally set up a loss function. Remember this is a minimization loop\n",
    "    # What's a simple loss function to maximize s/sqrt(b)\n",
    "\n",
    "    # Calculate gradients with backward pass\n",
    "\n",
    "    # Step with optimizer\n",
    "\n",
    "        \n",
    "    # Keep track of history\n",
    "    losses.append(loss.item())\n",
    "    a_history.append(a.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df838c91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()\n",
    "plt.plot(a_history)\n",
    "plt.show()\n",
    "print('Converged:', a_history[-1], -losses[-1])\n",
    "print('Sweep:', test_as[torch.argmax(all_sig)].item(), all_sig[torch.argmax(all_sig)].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f860af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.hist(m_s.numpy(), bins=20, range=[0, 2000], histtype='step')\n",
    "plt.hist(m_b.numpy(), bins=20, range=[0, 2000], histtype='step')\n",
    "\n",
    "plt.hist(all_ms.numpy(),  bins=20, range=[0, 2000], histtype='step')\n",
    "\n",
    "plt.plot([a_history[-1]]*2, [0, 1400])\n",
    "plt.plot([test_as[best_idx]]*2, [0, 1400])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108dc7e2",
   "metadata": {},
   "source": [
    "### Exercises for later:\n",
    "\n",
    "1) Impact of the rate: How big can you make the rate before the optimization breaks? What happens to accuracy as you make the rate smaller?\n",
    "\n",
    "2) In 1D this is really a toy problem, but what about in N-D? Consider a system defined by two masses, e.g.\n",
    "```\n",
    "    m_s1 = 1000*torch.normal(torch.ones(n_s)*0.6, torch.ones(n_s)*0.05)+500\n",
    "    m_s2 = 1000*torch.normal(torch.ones(n_s)*0.5, torch.ones(n_s)*0.1)+500\n",
    "    \n",
    "    m_b1 = 1000*torch.normal(torch.ones(n_b)*0.2, torch.ones(n_b)*0.3)+500\n",
    "    m_b2 = 1000*torch.normal(torch.ones(n_b)*0.1, torch.ones(n_b)*0.4)+500\n",
    "```\n",
    "Can you run the same sort of analysis?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c9afab",
   "metadata": {},
   "source": [
    "## Extensions: Differentiable Pipelines\n",
    "\n",
    "Writing differentiable physics code can be an excellent tool all on its own. However, linking physics code with other pieces of a machine learning pipeline, such as neural networks, can be a poweful tool as well. \n",
    "\n",
    "As a simple example, let's come back to our kinematic equation. Suppose we have a pitching machine that fires baseballs in a place with known acceleration, but with an initial velocity and position that become miscalibrated as the day goes on.\n",
    "\n",
    "Let's say we want to develop a prediction for the final position of the baseballs as a function of time of day.\n",
    "\n",
    "There are two ways one could do this: \n",
    "\n",
    "1) Train a model to go from time of day to final position\n",
    "\n",
    "2) Train a model to go from time of day to initial position + velocity, use our differentiable simulator to propagate to the final position. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b71e44",
   "metadata": {},
   "source": [
    "### What's the point?\n",
    "\n",
    "Let's reduce to 1D for simplicity. Assume "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6116de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Acceleration due to gravity (-9.8 m/s^2 in negative y direction)\n",
    "a = torch.tensor(-9.8)\n",
    "\n",
    "#Initial velocity for fun (m / s)\n",
    "v0_start = torch.tensor(10)\n",
    "\n",
    "#Start at 10 [m]\n",
    "x0_start = torch.tensor(15)\n",
    "\n",
    "# End time: assume this is fixed\n",
    "t_end = torch.tensor(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb9a491",
   "metadata": {},
   "source": [
    "We define the position function as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c1f009",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def position(t, a, x0, v0):\n",
    "    return x0 + v0*t + 0.5*a*t**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad864148",
   "metadata": {},
   "source": [
    "And now introduce some drift in v0 and x0. For simplicity, let's assume that t_launch (the time of day we're launching) runs from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e064a93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def v0_drift(t_launch):\n",
    "    return 5*t_launch**2\n",
    "\n",
    "def x0_drift(t_launch):\n",
    "    return 2*torch.sin(20*t_launch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a407dce6",
   "metadata": {},
   "source": [
    "Set up some training data for launching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4dcb4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_t_launch = torch.linspace(0, 1, 100)\n",
    "all_v0_drift = v0_drift(all_t_launch)\n",
    "all_x0_drift = x0_drift(all_t_launch)\n",
    "\n",
    "all_end = position(t_end, a, x0_start+all_x0_drift, v0_start+all_v0_drift)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd88c030",
   "metadata": {},
   "source": [
    "Now the ending position as a function of launch time is some convoluted function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d62d1bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(all_t_launch, all_end)\n",
    "plt.xlabel('Launch Time')\n",
    "plt.ylabel('Ending Position')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0da292b",
   "metadata": {},
   "source": [
    "But the drift functions are relatively simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4739ca5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(all_t_launch, all_x0_drift)\n",
    "plt.xlabel('Launch Time')\n",
    "plt.ylabel('x0 Drift')\n",
    "plt.show()\n",
    "plt.plot(all_t_launch, all_v0_drift)\n",
    "plt.xlabel('Launch Time')\n",
    "plt.ylabel('v0 Drift')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbdac45",
   "metadata": {},
   "source": [
    "### Claim: Modeling the underlying variables may make it easier to model the objective\n",
    "\n",
    "Let's test this out with some simple MLPs.\n",
    "\n",
    "\n",
    "First, let's set up a network to directly model the end position drift:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295dc320",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MLP_direct(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "                          nn.Linear(1, 100),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(100, 50),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(50, 1)\n",
    "                        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabb67b6",
   "metadata": {},
   "source": [
    "Our inputs are the launch times, targets are the end positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33c6d52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = all_t_launch[:, None]\n",
    "targets = all_end[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9274db48",
   "metadata": {},
   "source": [
    "Traning loop should be pretty standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef3af2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the MLP\n",
    "torch.random.manual_seed(3)\n",
    "mlp = MLP_direct()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-3)\n",
    "\n",
    "# Run the training loop\n",
    "losses = []\n",
    "predictions_direct = []\n",
    "for epoch in tqdm(range(4000)): \n",
    "\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Perform forward pass\n",
    "    outputs = mlp(inputs)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = loss_fn(outputs, targets)\n",
    "\n",
    "    # Perform backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Perform optimization\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    predictions_direct.append(outputs.flatten().clone().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64213565",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel('Training Iteration')\n",
    "plt.ylabel('Losses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537e03cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(all_t_launch, predictions_direct[-1], c='r', label='Fit')\n",
    "plt.plot(all_t_launch, all_end, c='k', label='True')\n",
    "plt.legend()\n",
    "plt.xlabel('Launch Time')\n",
    "plt.ylabel('End Position')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ba8eb9",
   "metadata": {},
   "source": [
    "Not too bad (and we could probably make it better with tweaking) but misses some fine structure!\n",
    "\n",
    "\n",
    "Now with differentiable programming, we model the drift and propagate it through:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bedfb3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MLP_drift(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "                          nn.Linear(1, 100),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(100, 50),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(50, 2)\n",
    "                        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0024991",
   "metadata": {},
   "source": [
    "Our inputs will be the launch times and our targets will still be the end positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b36898",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = all_t_launch[:, None]\n",
    "targets = all_end[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11182ff6",
   "metadata": {},
   "source": [
    "Training loop is the same, but with an extra step in the middle!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333ce8cf-f08e-42b2-b3b1-236f58096613",
   "metadata": {},
   "source": [
    "### Exercise 5: Fill in the extra step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6213fde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the MLP\n",
    "torch.random.manual_seed(3)\n",
    "mlp = MLP_drift()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-3)\n",
    "\n",
    "# Run the training loop\n",
    "losses = []\n",
    "pred_x0_drift = []\n",
    "pred_v0_drift = []\n",
    "final_pred_drift = []\n",
    "\n",
    "for epoch in tqdm(range(4000)): \n",
    "\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Perform forward pass to get drift\n",
    "    full_drift = mlp(inputs)\n",
    "    \n",
    "    x0_drift = full_drift[:, 0]\n",
    "    v0_drift = full_drift[:, 1]\n",
    "    \n",
    "    # Propagate drift through to end position\n",
    "    # Use the function position, as well as known\n",
    "    # t_end and a. Shifted x0 is x0_start+x0_drift, similar for v0\n",
    "    # For shape consistency, we'll need position(...)[:, None]\n",
    "\n",
    "    # Compute loss\n",
    "\n",
    "\n",
    "    # Perform backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Perform optimization\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    pred_x0_drift.append(x0_drift.flatten().clone().detach())\n",
    "    pred_v0_drift.append(v0_drift.flatten().clone().detach())\n",
    "    final_pred_drift.append(pred.flatten().clone().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0061f2d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8993a025",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(all_t_launch, final_pred_drift[-1], c='r', label='Fit')\n",
    "plt.plot(all_t_launch, all_end, c='k', label='True')\n",
    "plt.legend()\n",
    "plt.xlabel('Launch Time')\n",
    "plt.ylabel('End Position')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf97129",
   "metadata": {},
   "source": [
    "Not perfect, but much better! Let's see how the parameters look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4510c1b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(all_t_launch, pred_x0_drift[-1], c='r', label='Fit')\n",
    "plt.plot(all_t_launch, all_x0_drift, c='k', label='True')\n",
    "plt.xlabel('Launch Time')\n",
    "plt.ylabel('x0 Drift')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(all_t_launch, pred_v0_drift[-1], c='r', label='Fit')\n",
    "plt.plot(all_t_launch, all_v0_drift, c='k', label='True')\n",
    "plt.xlabel('Launch Time')\n",
    "plt.ylabel('v0 Drift')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d82f8e",
   "metadata": {},
   "source": [
    "**Cautionary tale:** model _degeneracies_ may lead to unexpected results! But including physics can still help (e.g. extra (physical) degrees of freedom). Degeneracies may be broken with _additional constraints_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a818f4f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-2.0.1",
   "language": "python",
   "name": "pytorch-2.0.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
